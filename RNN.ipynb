{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c75475ea-3209-44a7-a566-3afd988ac35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e47fc7-b846-4edb-b36c-310415b13e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    # Weights\n",
    "    W = []\n",
    "    # Layers\n",
    "    M = []\n",
    "    # Bias\n",
    "    b = []\n",
    "    # Dropout percentage\n",
    "    D = []\n",
    "    # Activation function for each layer\n",
    "    A = []\n",
    "    Yhat = []\n",
    "    costs = []\n",
    "    lossType = 'ce'\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def __sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    def __tanh(self, Z):\n",
    "        return np.tanh(Z)\n",
    "    \n",
    "    def __relu(self,Z):\n",
    "        return Z * (Z > 0)\n",
    "    \n",
    "    # Calls specified activation function\n",
    "    def __actf(self,Z,ty):\n",
    "        if ty == 'sigmoid':\n",
    "            return self.__sigmoid(Z)\n",
    "        elif ty == 'tanh':\n",
    "            return self.__tanh(Z)\n",
    "        elif ty == 'softmax':\n",
    "            return self.__softmax(Z)\n",
    "        elif ty == 'none':\n",
    "            return Z\n",
    "        else:# ty == 'relu':\n",
    "            return self.__relu(Z)\n",
    "        \n",
    "        # Calls specified activation function derivative\n",
    "    def __actf_dv(self,Z,ty):\n",
    "        if ty == 'sigmoid':\n",
    "            return Z*(1-Z)\n",
    "        elif ty == 'tanh':\n",
    "            return (1-Z*Z)\n",
    "        else:# ty == 'relu':\n",
    "            return np.where(Z > 0, 1, 0)\n",
    "        \n",
    "    def __softmax(self,A):\n",
    "        expA = np.exp(A)\n",
    "        return expA / expA.sum(axis=1,keepdims=True)\n",
    "    \n",
    "    # Cross-entropy cost for softmax\n",
    "    def __cost(self,T,Y,ty):\n",
    "        if ty == 'ce':\n",
    "            return self.__cross_entropy(T,Y)\n",
    "        else:\n",
    "            return self.__sparse_cat(T,Y)\n",
    "    \n",
    "    def __cross_entropy(self,T,Y):\n",
    "        tot = (-T * np.log(Y))\n",
    "        return tot.sum()\n",
    "    \n",
    "    # Sparse categorical cross-entropy loss\n",
    "    def __sparse_cat(self, T, Y):\n",
    "        tot = 0\n",
    "        for i in range(len(Y)):\n",
    "            tot += -np.log(Y[i][T[i]])\n",
    "        return tot;\n",
    "    \n",
    "    def classification_rate(self,T):\n",
    "        Yhat = NeuralNetwork.Yhat\n",
    "        Yp = np.argmax(Yhat,axis=1)\n",
    "        print('Classification rate: ', np.mean(T == Yp))\n",
    "    \n",
    "    # Adds hidden layer with L nodes, d dropout\n",
    "    def add_layer(self,L,a='sigmoid',d=0):\n",
    "        self.M.append(L)\n",
    "        self.A.append(a)\n",
    "        d = min(d,1)\n",
    "        d = max(d, 0)\n",
    "        self.D.append(d)\n",
    "        \n",
    "    def __shuffle(self,X,Y,y):\n",
    "        assert len(X) == len(Y) == len(y)\n",
    "        p = np.random.permutation(len(X))\n",
    "        return X[p],Y[p],y[p]\n",
    "        \n",
    "            \n",
    "    # Parameters(M:Layers,W:Weights,b:bias,A:activation function,D:Dropout)\n",
    "    def __forward(self,M,W,b,A,D):\n",
    "        for i in range(1,len(W)+1):\n",
    "            #if (i != len(W)+1):\n",
    "            M[i] = self.__actf((M[i-1].dot(W[i-1]) + b[i-1]), A[i-1])\n",
    "            # Dropout\n",
    "            if (D[i-1] > 0):\n",
    "                for j in range(len(M[i])):\n",
    "                    if (random.random() < D[i-1]):\n",
    "                        M[i][j] = 0\n",
    "            #else:\n",
    "            #    M[i] = M[i-1].dot(W[i-1]) + b[i-1]\n",
    "            #    Y = self.__softmax(M[i])\n",
    "        Y = M[-1]\n",
    "        return Y,M\n",
    "    \n",
    "    def fit(self,X,y,epochs=20000,batchSize=0,learnR=10e-6,reg=0,lossType='ce',optimizer='none'):\n",
    "        W = self.W\n",
    "        b = self.b\n",
    "        M = self.M\n",
    "        A = self.A\n",
    "        D = self.D\n",
    "        #NeuralNetwork.actf = a\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Initialize layers for M\n",
    "        N = X.shape[0]\n",
    "        for i in range(len(M)):\n",
    "            M[i] = np.random.randn(N,M[i])\n",
    "            \n",
    "        # Add input and output layers to M\n",
    "        K = len(set(y))\n",
    "        M.insert(0,X)\n",
    "        M.append(np.random.randn(N,K))\n",
    "        # Add softmax to end of activation functions\n",
    "        A.append('softmax')\n",
    "        D.append(0)\n",
    "        l = learnR\n",
    "        \n",
    "        # Regulate batch size\n",
    "        batchSize = min(batchSize, N)\n",
    "        batchSize = max(batchSize, 1)\n",
    "        \n",
    "        # Set indicator matrix\n",
    "        Y = np.zeros((N,K))\n",
    "        for i in range(N):\n",
    "            Y[i,y[i]] = 1\n",
    "            \n",
    "        # Set weights\n",
    "        for i in range(len(M)-1):\n",
    "            if i == (len(M)-2):\n",
    "                W.append(np.random.randn(M[i].shape[1],K) / np.sqrt(M[i].shape[1] + K))\n",
    "                b.append(np.random.randn(K) / np.sqrt(K))\n",
    "            else:\n",
    "                W.append(np.random.randn(M[i].shape[1],M[i+1].shape[1]) / np.sqrt(M[i].shape[1] + M[i+1].shape[1]))\n",
    "                b.append(np.random.randn(M[i+1].shape[1]) / np.sqrt(M[i+1].shape[1]))\n",
    "        # Set cache (if using rmsprop/adam)\n",
    "        if optimizer == 'rms':\n",
    "            wCache = []\n",
    "            bCache = []\n",
    "            eps = 10e-8\n",
    "            decay = 0.99\n",
    "            for i in range(len(M)-1):\n",
    "                if i == (len(M)-2):\n",
    "                    wCache.append(np.ones((M[i].shape[1],K)))\n",
    "                    bCache.append(np.ones((K)))\n",
    "                else:\n",
    "                    wCache.append(np.ones((M[i].shape[1],M[i+1].shape[1])))\n",
    "                    bCache.append(np.ones((M[i+1].shape[1])))\n",
    "        elif optimizer == 'adam':\n",
    "            wM = []\n",
    "            wV = []\n",
    "            bM = []\n",
    "            bV = []\n",
    "            eps = 10e-8\n",
    "            decay1 = 0.99\n",
    "            decay2 = 0.999\n",
    "            for i in range(len(M)-1):\n",
    "                if i == (len(M)-2):\n",
    "                    wM.append(np.ones((M[i].shape[1],K)))\n",
    "                    wV.append(np.ones((M[i].shape[1],K)))\n",
    "                    bM.append(np.ones((K)))\n",
    "                    bV.append(np.ones((K)))\n",
    "                else:\n",
    "                    wM.append(np.ones((M[i].shape[1],M[i+1].shape[1])))\n",
    "                    wV.append(np.ones((M[i].shape[1],M[i+1].shape[1])))\n",
    "                    bM.append(np.ones((M[i+1].shape[1])))\n",
    "                    bV.append(np.ones((M[i+1].shape[1])))\n",
    "        \n",
    "        costs = []\n",
    "        for e in range(epochs):\n",
    "            iterations = N // batchSize\n",
    "            X,Y,y = self.__shuffle(X,Y,y)\n",
    "            for i in range(iterations):\n",
    "                start = i * batchSize\n",
    "                end = (i+1) * batchSize\n",
    "                batchX, batchY, sparseY = X[start:end],Y[start:end],y[start:end]\n",
    "                del M[0]\n",
    "                M.insert(0,batchX)\n",
    "                #print(sparceY)\n",
    "                Yp,Z = self.__forward(M,W,b,A,D)\n",
    "                #cost = self.__cost(Y,Yp)\n",
    "                if lossType == 'ce':\n",
    "                    cost = self.__cost(batchY,Yp,lossType)\n",
    "                else:\n",
    "                    cost = self.__cost(sparseY,Yp,lossType)\n",
    "                costs.append(cost)\n",
    "                \n",
    "\n",
    "                # Adjust weights\n",
    "                #S = (Y - Yp)\n",
    "                S = (batchY - Yp)\n",
    "                n = len(M)-2\n",
    "                Zt = S\n",
    "                for i in range(len(M)-1):\n",
    "                    # Weight and bias derivative\n",
    "                    dw = Z[n].T.dot(Zt)\n",
    "                    db = Zt.sum()\n",
    "                    if optimizer == 'none': \n",
    "                        W[n] += l * (dw - reg*W[n])\n",
    "                        b[n] += l * (db - reg*b[n])\n",
    "                    elif optimizer == 'rms':\n",
    "                        # rmsprop\n",
    "                        wCache[n] = (decay * wCache[n]) + (1-decay) * np.square(dw)\n",
    "                        bCache[n] = (decay * bCache[n]) + (1-decay) * np.square(db)\n",
    "                        wDenominator = np.sqrt(wCache[n]) + eps\n",
    "                        bDenominator = np.sqrt(bCache[n]) + eps\n",
    "                        \n",
    "                        W[n] += l * ((dw/wDenominator) - reg*W[n])\n",
    "                        b[n] += l * ((db/bDenominator) - reg*b[n])\n",
    "                    elif optimizer == 'adam':\n",
    "                        \n",
    "                        #print(wM.shape)\n",
    "                        wM[n] = (decay1 * wM[n]) + (1-decay1) * dw\n",
    "                        wV[n] = (decay2 * wV[n]) + (1-decay2) * np.square(dw)\n",
    "                        bM[n] = (decay1 * bM[n]) + (1-decay1) * db\n",
    "                        bV[n] = (decay2 * bV[n]) + (1-decay2) * np.square(db)\n",
    "                        wMhat = wM[n]/(1-decay1**(epochs+1))\n",
    "                        wVhat = wV[n]/(1-decay2**(epochs+1))\n",
    "                        bMhat = bM[n]/(1-decay1**(epochs+1))\n",
    "                        bVhat = bV[n]/(1-decay2**(epochs+1))\n",
    "                        wDenom = np.sqrt(wVhat) + eps\n",
    "                        bDenom = np.sqrt(bVhat) + eps\n",
    "                        #print(W[n].shape, wM[n].shape, wDenom.shape)\n",
    "                        W[n] += l * ((wMhat/wDenom) - reg*W[n])\n",
    "                        b[n] += l * ((bMhat/bDenom) - reg*b[n])\n",
    "\n",
    "                    if i != (len(M)-2):\n",
    "                        #Update Zt\n",
    "                        Zt = Zt.dot(W[n].T)*self.__actf_dv(Z[n],A[n-1])\n",
    "\n",
    "                    n -= 1\n",
    "            if e % 1000 == 0:\n",
    "                print(e,costs[-1])\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.M = M\n",
    "        self.Yhat = Yp\n",
    "        self.costs = costs\n",
    "    \n",
    "    def predict(self,X):\n",
    "        W = self.W\n",
    "        b = self.b\n",
    "        M = self.M\n",
    "        A = self.A\n",
    "        #a = NeuralNetwork.actf\n",
    "        D = self.D\n",
    "        del M[0]\n",
    "        M.insert(0,X)\n",
    "        Yp,Z = self.__forward(M,W,b,A,D)\n",
    "        NeuralNetwork.Yhat = Yp\n",
    "        return Yp\n",
    "    \n",
    "    def plot_cost(self):\n",
    "        costs = self.costs\n",
    "        plt.plot(costs)\n",
    "        plt.show()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        self.M = []\n",
    "        self.A = []\n",
    "        self.D = []\n",
    "        self.lossType = 'ce'\n",
    "        self.optimizer = 'none'\n",
    "        self.Yhat = []\n",
    "        self.costs = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
